---
title: "Coursera Capstone Project Milestone Report"
author: "SN de Koning"
date: "30 april 2016"
output: html_document
---

### Introduction

This report will document the steps of the Coursera Data Specialization Capstone-project taken up until this point. The goal of this project is to build an English-language text prediction model based on three different english text-files and create a Shiny app which uses this model to give text suggestions in response to user input. 

### Data Description

To build this model, three different files from the [HC Corpora](www.corpora.heliohost.org) were used. One textfile with news messages, one with text from weblogs, and one with text from twitter messages. For more info see the readme at the [source](http://www.corpora.heliohost.org/aboutcorpus.html). The link for the data was provided by Coursera.

```{r}
if(!dir.exists("./data")){
    dir.create("./data")
}    

if(!file.exists("./data/Coursera_Swiftkey.zip")){
    
   get_url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
   download.file(get_url, "./data/Coursera_Swiftkey.zip")
      unzip("./data/Coursera_Swiftkey.zip", exdir = "./data")
  
}
```


### Dependencies

```{r, message=FALSE, warning=FALSE}
library(tm)
library(ggplot2)
library(magrittr)
library(stringr)
library(RWeka)
```

### Data Transformations

```{r}
setwd("./data/final/en_US")


# Sampling 5% of the lines of the twitter file.
file_name <- "en_US.twitter.txt"
con <- file(file_name, open = "r")
twitter_lines <- readLines(con, encoding = "UTF-8")
close(con)

set.seed(290420161)

twitter_lines_sample <- sample(twitter_lines, size = length(twitter_lines)*.05)

save(twitter_lines_sample, file = "twitter_lines_sample.RData")
rm(list = ls())
gc(verbose = FALSE)

# Sampling 5% of the lines in the news file.
file_name <- "en_US.news.txt"
con <- file(file_name, open = "r")
news_lines <- readLines(con, encoding = "UTF-8")
close(con)

set.seed(290420162)

news_lines_sample <- sample(news_lines, size = length(news_lines)*.05)

save(news_lines_sample, file = "news_lines_sample.RData")
rm(list = ls())
gc(verbose = FALSE)

# Sampling 5% of the lines in the blog file.
file_name <- "en_US.blogs.txt"
con <- file(file_name, open = "r")
blog_lines <- readLines(con, encoding = "UTF-8")
close(con)

set.seed(290420163)

blog_lines_sample <- sample(blog_lines, size = length(blog_lines)*.05)

save(blog_lines_sample, file = "blog_lines_sample.RData")
rm(list = ls())
gc(verbose = FALSE)
```

```{r}

total_sample <- paste(blog_lines_sample, news_lines_sample, twitter_lines_sample)
rm(blog_lines_sample, news_lines_sample, twitter_lines_sample)

doc_corpus <- Corpus(VectorSource(total_sample))

# Getting bad words.

bad_words <- scan("./bad_words.txt", what = "character", sep = "\n")

# Cleaning up the corpus.
remove_url <- function(x) {
    gsub("http[[:alnum:]]*", "", x)
    }

doc_corpus_clean <- doc_corpus %>% 
    tm_map(content_transformer(removePunctuation)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(content_transformer(removeNumbers)) %>%
    tm_map(content_transformer(remove_url)) %>%
    tm_map(stripWhitespace) %>%
    tm_map(removeWords, bad_words) %>%
    tm_map(stripWhitespace)

save(doc_corpus_clean, "./data/doc_corpus_clean.RData")

doc_term_matrix <- DocumentTermMatrix(doc_corpus_clean)
sparse_dtm <- removeSparseTerms(doc_term_matrix, sparse =  0.99)

save(doc_term_matrix, file = "./data/doc_term_matrix.RData")
save(sparse_dtm, file = "./data/sparse_dtm.Rdata")
```

### Explorations

### N-Grams

### Further Steps