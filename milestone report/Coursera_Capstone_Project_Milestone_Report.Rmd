---
title: "Coursera Capstone Project Milestone Report"
author: "SN de Koning"
date: "30 april 2016"
output: html_document
---

### Introduction

This report will document the steps of the Coursera Data Specialization Capstone-project taken up until this point. The goal of this project is to build an English-language text prediction model based on three different english text-files and create a Shiny app which uses this model to give text suggestions in response to user input. 

### Data Description

To build this model, three different files from the [HC Corpora](www.corpora.heliohost.org) were used. One textfile with news messages, one with text from weblogs, and one with text from twitter messages. For more info see the readme at the [source](http://www.corpora.heliohost.org/aboutcorpus.html). The link for the data was provided by Coursera.

```{r}
if(!dir.exists("./data")){
    dir.create("./data")
}    

if(!file.exists("./data/Coursera_Swiftkey.zip")){
    
   get_url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
   download.file(get_url, "./data/Coursera_Swiftkey.zip")
      unzip("./data/Coursera_Swiftkey.zip", exdir = "./data")
  
}
```


### Dependencies

To conduct the analyses the following packages were utilized. As the project progresses, it is likely more packages are to be added.

```{r, message=FALSE, warning=FALSE}
library(tm)
library(ggplot2)
library(magrittr)
library(stringr)
library(RWeka)
```

### Data Transformations

Since the files are quite large, samples were made to build the model on.

```{r, message=FALSE, warning=FALSE}
# Reading and sampling the twitter file.
if(!file.exists("./data/final/en_US/twitter_lines_sample.RData")){
    file_name <- "./data/final/en_US/en_US.twitter.txt"
    con <- file(file_name, open = "r")
    twitter_lines <- readLines(con, encoding = "UTF-8")
    close(con)

    set.seed(290420161)
    twitter_lines_sample <- sample(twitter_lines, size = length(twitter_lines)*.05)
    save(twitter_lines_sample, file = "./data/final/en_US/twitter_lines_sample.RData")
    rm(twitter_lines)
}else{
    load(file = "./data/final/en_US/twitter_lines_sample.RData")
}

```
```{r, message=FALSE, warning=FALSE}
# Reading and sampling the news file.
if(!file.exists("./data/final/en_US/news_lines_sample.RData")){
    file_name <- "./data/final/en_US/en_US.news.txt"
    con <- file(file_name, open = "r")
    news_lines <- readLines(con, encoding = "UTF-8")
    close(con)

    set.seed(290420162)
    news_lines_sample <- sample(news_lines, size = length(news_lines)*.05)
    save(news_lines_sample, file = "./data/final/en_US/news_lines_sample.RData")
    rm(news_lines)
}else{
    load(file = "./data/final/en_US/news_lines_sample.RData")
}
```
```{r, message=FALSE, warning=FALSE}
# Reading and sampling the blog file.
if(!file.exists("./data/final_en_US/blog_lines_sample.RData")){
    file_name <- "./data/final/en_US/en_US.blogs.txt"
    con <- file(file_name, open = "r")
    blog_lines <- readLines(con, encoding = "UTF-8")
    close(con)

    set.seed(290420163)
    blog_lines_sample <- sample(blog_lines, size = length(blog_lines)*.05)
    save(blog_lines_sample, file = "./data/final/en_US/blog_lines_sample.RData")
    rm(blog_lines)
}else{
    load(file = "./data/final/en_US/blog_lines_sample.RData")
}
```

```{r, cache=TRUE}
total_sample <- paste(blog_lines_sample, news_lines_sample, twitter_lines_sample)
total_sample <- iconv(total_sample, from = "UTF-8", to = "latin1", sub = "")
rm(blog_lines_sample, news_lines_sample, twitter_lines_sample)
```

The total sample consisted of `r length(total_sample)` lines of text, with `r sum(unique(str_count(total_sample, pattern = boundary("word"))))` unique words. Do note that included in the unique words are also non-dictionary words, such as names and onomatopoeias (e.g.: "argh"), expletives and misspelled words.
Expletives and obscene words were filtered for the sake of this project.

```{r, cache=TRUE}
#Converting to a corpus format
doc_corpus <- Corpus(VectorSource(total_sample))

# Getting the expletives filter.
bad_words <- scan("./bad_words.txt", what = "character", sep = "\n")

# Function to remove url's from the strings.
remove_url <- function(x) {
    gsub("http[[:alnum:]]*", "", x)
    }

# Cleaning up the corpus, getting rid of punctuation, numbers, urls, profanity.
doc_corpus_clean <- doc_corpus %>%
    tm_map(content_transformer(removePunctuation)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(content_transformer(removeNumbers)) %>%
    tm_map(content_transformer(remove_url)) %>%
    tm_map(stripWhitespace) %>%
    tm_map(removeWords, bad_words) %>%
    tm_map(stripWhitespace)

doc_term_matrix <- DocumentTermMatrix(doc_corpus_clean)

```

### Explorations

The plot below shows the top 50 most used words in this sample of the corpora. As to be expected words such as "the" and "and" are at the top of this list.

```{r}
# Removing sparse terms, i.e. words that occur very infrequently.
sparse_dtm <- removeSparseTerms(doc_term_matrix, sparse =  0.99)

# Sorting by frequency.
word_frequencies <- sort(colSums(as.matrix(sparse_dtm)), decreasing = TRUE)
word_frequencies_df <- data.frame(word = names(word_frequencies),
                                  frequency = word_frequencies)

word_frequencies_df$word <- factor(word_frequencies_df$word,
                                   levels = word_frequencies_df$word)
# Barplot of word frequencies.
ggplot(word_frequencies_df[1:50, ,], aes(x=word, y=frequency)) +
    geom_bar(stat="identity") +
    xlab("") +
    ylab("Frequency") +
    ggtitle("Top 50 most frequently occurring words over the three datasets") +
    coord_flip()
```

### N-Grams

To find words commonly used together, the rWeka package provides functionality to tokenize the strings into any n-gram specified. Which for the sake of brevity of this report was limited to 2-grams.

```{r, cache=TRUE}
# Function to tokenize into 2grams. 
tokenizer_function <- function(x) {
    NGramTokenizer(x, Weka_control(min = 2, max = 4))
}

tokenized_dtm <- DocumentTermMatrix(doc_corpus_clean,
                                    control = list(tokenize = tokenizer_function))
```

Similar to the word frequency plot, we can get rid of the sparse terms if we only need the most frequent 2grams.

```{r}
# Removing sparse terms.
sparse_ngram_dtm <- removeSparseTerms(tokenized_dtm, sparse = .99)

# Sorting by frequency.
twogram_frequencies <- sort(colSums(as.matrix(sparse_ngram_dtm)),
                            decreasing = TRUE)
twogram_frequencies_df <- data.frame(twogram = names(twogram_frequencies),
                                     frequency = twogram_frequencies)

twogram_frequencies_df$twogram <- factor(twogram_frequencies_df$twogram,
                                   levels = twogram_frequencies_df$twogram)

# Barplot of 2gram frequencies.
ggplot(twogram_frequencies_df[1:50, ,], aes(x=twogram, y=frequency)) +
    geom_bar(stat="identity") +
    xlab("") +
    ylab("Frequency") +
    ggtitle("Top 50 most frequently occurring 2-Gram word combinations over the three datasets") +
    coord_flip()
```


### Further Steps.

One of the challenges to tackle after this, is how to handle "new" words, that is, words that haven't been found in the corpora. One way could be setting "default" words, highly frequent words as a follow-up to unknown words.
Other considerations are incorperating 3 and 4 grams in the model for better text prediction.